{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Proabability and Bayes Rule](#proabability-and-bayes-rule)\n",
    "- [Naive Bayes for Sentiment Analysis](#naive-bayes-for-sentiment-analysis)\n",
    "- [Laplacian Smoothing](#laplacian-smoothing)\n",
    "- [Log Likelihood](#log-likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proabability and Bayes Rule\n",
    "\n",
    "Proability is the frequency of a certain event. If we have a corpus of tweets we can calculate the positive and nevative probability P(A).\n",
    "\n",
    "![Logistic Regression](images/probabilities.png)\n",
    "\n",
    "We can also definite different event P(B), for example the probability that the word contains happy\n",
    "\n",
    "The proability that an event is both positive and happy is then the intersection between the positive and happy set\n",
    "\n",
    "![Adjoint Probability](images/adjoint_probability.png)\n",
    "\n",
    "Now let s condider only the happy words of the corpus. The proability goes much higher for this calculation (75% for the example). You can do the same asy for example for the proability that happy is positive. \n",
    "\n",
    "We talk here of conditional probability, or the probability of B given that A happened or given looking at elements of set A, the probaily that is belongs to B as well.\n",
    "Conditional probabilities help us reduce the sample search space. For example given a specific event already happened, i.e. we know the word is happy:\n",
    "\n",
    "![Bayes Venn Diagram](images/bayes_venn_diagram.png)\n",
    "\n",
    "Then you would only search in the blue circle above. The numerator will be the red part and the denominator will be the blue part. This leads us to conclude the following: \n",
    "$$\n",
    "P(\\text{Positive} \\mid \\text{``happy''}) = \\frac{P(\\text{Positive} \\cap \\text{``happy''})}{P(\\text{``happy''})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{``happy''} \\mid \\text{Positive}) = \\frac{P(\\text{``happy''} \\cap \\text{Positive})}{P(\\text{Positive})}\n",
    "$$\n",
    "\n",
    "And since $P(\\text{``happy''} \\cap \\text{Positive}) = P(\\text{Positive} \\cap \\text{``happy''})$\n",
    "\n",
    "$$\n",
    "P(\\text{Positive} \\mid \\text{``happy''}) = P(\\text{``happy''} \\mid \\text{Positive}) \\times \\frac{P(\\text{Positive})}{P(\\text{``happy''})}\n",
    "$$\n",
    "\n",
    "Given us the basic bayes equation\n",
    "\n",
    "$$\n",
    "P(X \\mid Y) = \\frac{P(Y \\mid X) P(X)}{P(Y)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Sentiment Analysis\n",
    "\n",
    "Similar to before, you will begin with two corpus. One for the positive tweets and one for the negative tweets. All the different words that appear in your corpus, along with their counts for positive and negative. \n",
    "\n",
    "For each word you can calculate the conditional probability of each word within the positive and negative class. You can find \"power words\", or words that have statistically different probability is one or the other category.\n",
    "<center>\n",
    "<img src=\"images/word_conditional_probability.png\" width=\"250\" alt=\"Conditional Word Probability\"/>\n",
    "</center>\n",
    "\n",
    "Sometimes $P(w_i | \\text{class})$ will yield a value of zero, which will make comparisons not possible. You can solve this problem by applying some smoothing process. This expression is called the Naive Bayes inference condition rule for binary classification. This is calculated as $$\\prod_{i=1}^m \\frac{P(w_i|pos)}{P(w_i|neg)}$$\n",
    "\n",
    "Using this calculation you will get values >1 for positive sentiment analysis and <1 for negative sentiment analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian Smoothing\n",
    "\n",
    "Sometimes you might end up having words that never show up in your corpus. You get a probability of zero, and the probability of an entire sequence might go to zero. You can use a technique you can use to avoid your probabilities being zero.\n",
    "\n",
    "Instead of calculating $$P(w_i|\\text{class}) = \\frac{\\text{freq}(w_i, \\text{class})}{N_\\text{class}} \\quad \\text{class} \\in \\{\\text{Positive}, \\text{Negative}\\}$$\n",
    "\n",
    "You can add a one in the numerator and add at the denominator all of the unique words in your entire vocabulary\n",
    "\n",
    "$$P(w_i|\\text{class}) = \\frac{\\text{freq}(w_i, \\text{class}) + 1}{N_\\text{class} + V_\\text{vocabulary}} \\\\[1em]\n",
    "\n",
    "N_\\text{class} = \\text{frequency of all words in class} \\\\[1em]\n",
    "\n",
    "V_\\text{vocabulary} = \\text{number of unique words in vocabulary}$$\n",
    "\n",
    "With this the sum of probabilities should still sum to 1\n",
    "\n",
    "So for a simple example we will have\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>word</th><th>Pos</th><th>Neg</th>\n",
    "        <th style=\"border: none; width: 20px;\"></th>\n",
    "        <th>word</th><th>Pos</th><th>Neg</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>I</td><td>3</td><td>3</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>I</td><td>0.19</td><td>0.20</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>am</td><td>3</td><td>3</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>am</td><td>0.19</td><td>0.20</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>happy</td><td>2</td><td>1</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>happy</td><td>0.14</td><td>0.10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>because</td><td>1</td><td>0</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>because</td><td>0.10</td><td>0.05</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>learning</td><td>1</td><td>1</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>learning</td><td>0.10</td><td>0.10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>NLP</td><td>1</td><td>1</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>NLP</td><td>0.10</td><td>0.10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>sad</td><td>1</td><td>2</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>sad</td><td>0.10</td><td>0.15</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>not</td><td>1</td><td>2</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>not</td><td>0.10</td><td>0.15</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Nclass</b></td><td><b>13</b></td><td><b>13</b></td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td><b>Sum</b></td><td><b>1</b></td><td><b>1</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<div align=\"left\">\n",
    "    <strong>Laplacian Smoothing</strong><br>\n",
    "    V = 8\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntlk_sentiment_analysis-qshMIsGW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
