{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Proabability and Bayes Rule](#proabability-and-bayes-rule)\n",
    "- [Naive Bayes for Sentiment Analysis](#naive-bayes-for-sentiment-analysis)\n",
    "- [Laplacian Smoothing](#laplacian-smoothing)\n",
    "- [Log-Likelihood](#log-likelihood)\n",
    "- [Train Naive Bayes](#train-naive-bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proabability and Bayes Rule\n",
    "\n",
    "Proability is the frequency of a certain event.\n",
    "If we have a corpus of tweets we can calculate the positive and nevative probability P(A).\n",
    "\n",
    "<center>\n",
    "<img src=\"images/probabilities.png\" width=\"750\" alt=\"Conditional Word Probability\"/>\n",
    "</center>\n",
    "\n",
    "We can also definite different event P(B), for example the probability that the word contains happy\n",
    "\n",
    "The proability that an event is both positive and happy is then the intersection between the positive and happy set\n",
    "\n",
    "<center>\n",
    "<img src=\"images/adjoint_probability.png\" width=\"750\" alt=\"Conditional Word Probability\"/>\n",
    "</center>\n",
    "\n",
    "Now let s condider only the happy words of the corpus.\n",
    "The proability goes much higher for this calculation (75% for the example). You can do the same asy for example for the proability that happy is positive. \n",
    "\n",
    "We talk here of conditional probability, or the probability of B given that A happened or given looking at elements of set A, the probaily that is belongs to B as well.\n",
    "Conditional probabilities help us reduce the sample search space. For example given a specific event already happened, i.e. we know the word is happy:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/bayes_venn_diagram.png\" width=\"750\" alt=\"Conditional Word Probability\"/>\n",
    "</center>\n",
    "\n",
    "Then you would only search in the blue circle above.\n",
    "The numerator will be the red part and the denominator will be the blue part.\n",
    "This leads us to conclude the following:\n",
    "$$\n",
    "P(\\text{Positive} \\mid \\text{``happy''}) = \\frac{P(\\text{Positive} \\cap \\text{``happy''})}{P(\\text{``happy''})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{``happy''} \\mid \\text{Positive}) = \\frac{P(\\text{``happy''} \\cap \\text{Positive})}{P(\\text{Positive})}\n",
    "$$\n",
    "\n",
    "And since $P(\\text{``happy''} \\cap \\text{Positive}) = P(\\text{Positive} \\cap \\text{``happy''})$\n",
    "\n",
    "$$\n",
    "P(\\text{Positive} \\mid \\text{``happy''}) = P(\\text{``happy''} \\mid \\text{Positive}) \\times \\frac{P(\\text{Positive})}{P(\\text{``happy''})}\n",
    "$$\n",
    "\n",
    "Given us the basic bayes equation\n",
    "\n",
    "$$\n",
    "P(X \\mid Y) = \\frac{P(Y \\mid X) P(X)}{P(Y)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Sentiment Analysis\n",
    "\n",
    "Similar to before, you will begin with two corpus. One for the positive tweets and one for the negative tweets.\n",
    "All the different words that appear in your corpus, along with their counts for positive and negative. \n",
    "\n",
    "For each word you can calculate the conditional probability of each word within the positive and negative class.\n",
    "You can find \"power words\", or words that have statistically different probability is one or the other category.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/word_conditional_probability.png\" width=\"250\" alt=\"Conditional Word Probability\"/>\n",
    "</center>\n",
    "\n",
    "Sometimes $P(w_i | \\text{class})$ will yield a value of zero, which will make comparisons not possible.\n",
    "You can solve this problem by applying some smoothing process.\n",
    "This expression is called the Naive Bayes inference condition rule for binary classification.\n",
    "This is calculated as $$\\prod_{i=1}^m \\frac{P(w_i|pos)}{P(w_i|neg)}$$\n",
    "\n",
    "Using this calculation you will get values >1 for positive sentiment analysis and <1 for negative sentiment analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian Smoothing\n",
    "\n",
    "Sometimes you might end up having words that never show up in your corpus. You get a probability of zero, and the probability of an entire sequence might go to zero.\n",
    "You can use a technique you can use to avoid your probabilities being zero.\n",
    "\n",
    "Instead of calculating $$P(w_i|\\text{class}) = \\frac{\\text{freq}(w_i, \\text{class})}{N_\\text{class}} \\quad \\text{class} \\in \\{\\text{Positive}, \\text{Negative}\\}$$\n",
    "\n",
    "You can add a one in the numerator and add at the denominator all of the unique words in your entire vocabulary\n",
    "\n",
    "$$P(w_i|\\text{class}) = \\frac{\\text{freq}(w_i, \\text{class}) + 1}{N_\\text{class} + V_\\text{vocabulary}} \\\\[1em]\n",
    "\n",
    "N_\\text{class} = \\text{frequency of all words in class} \\\\[1em]\n",
    "\n",
    "V_\\text{vocabulary} = \\text{number of unique words in vocabulary}$$\n",
    "\n",
    "With this the sum of probabilities should still sum to 1\n",
    "\n",
    "So for a simple example we will have\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>word</th><th>Pos</th><th>Neg</th>\n",
    "        <th style=\"border: none; width: 20px;\"></th>\n",
    "        <th>word</th><th>Pos</th><th>Neg</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>I</td><td>3</td><td>3</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>I</td><td>0.19</td><td>0.20</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>am</td><td>3</td><td>3</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>am</td><td>0.19</td><td>0.20</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>happy</td><td>2</td><td>1</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>happy</td><td>0.14</td><td>0.10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>because</td><td>1</td><td>0</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>because</td><td>0.10</td><td>0.05</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>learning</td><td>1</td><td>1</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>learning</td><td>0.10</td><td>0.10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>NLP</td><td>1</td><td>1</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>NLP</td><td>0.10</td><td>0.10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>sad</td><td>1</td><td>2</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>sad</td><td>0.10</td><td>0.15</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>not</td><td>1</td><td>2</td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td>not</td><td>0.10</td><td>0.15</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Nclass</b></td><td><b>13</b></td><td><b>13</b></td>\n",
    "        <td style=\"border: none;\"></td>\n",
    "        <td><b>Sum</b></td><td><b>1</b></td><td><b>1</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<div align=\"left\">\n",
    "    <strong>Laplacian Smoothing</strong><br>\n",
    "    V = 8\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Likelihood\n",
    "\n",
    "For each word you can calculate the ratio of the probabilities as $\\text{ratio}(w_i) = \\frac{P(w_i|\\text{Pos})}{P(w_i|\\text{Neg})}$\n",
    "\n",
    "| word      | Pos  | Neg  | ratio |\n",
    "|-----------|------|------|-------|\n",
    "| I         | 0.20 | 0.20 | 1     |\n",
    "| am        | 0.20 | 0.20 | 1     |\n",
    "| **happy** | 0.14 | 0.10 | **1.4** |\n",
    "| because   | 0.10 | 0.10 | 1     |\n",
    "| learning  | 0.10 | 0.10 | 1     |\n",
    "| NLP       | 0.10 | 0.10 | 1     |\n",
    "| **sad**   | 0.10 | 0.15 | **0.6** |\n",
    "| **not**   | 0.10 | 0.15 | **0.6** |\n",
    "\n",
    "This will help you to outline the positivity or negativity of a single word. \n",
    "\n",
    "### Naive Bayes' inference\n",
    "\n",
    "Previously we have calculated the likelihood of a single sentence as the product of the probability of a word being positive divided by the probability of a word being negative.\n",
    "However if your sample is not perfectly balanced (equal number of positive and negative tweets), you will have to take into account also the prior ratio of how balanced the set is\n",
    "$$\n",
    "\\frac{P(\\text{pos})}{P(\\text{neg})} \\cdot \\prod_{i=1}^{m} \\frac{P(w_i|\\text{pos})}{P(w_i|\\text{neg})} > 1\n",
    "$$\n",
    "\n",
    "Likelihood usually has the problem of going into underflow in normal computation, since you are multiplying a series of number smaller than one.\n",
    "The problem can be solved bby using the log transformation an using the properties of the logarithm.\n",
    "\n",
    "$$\n",
    "\\log(\\frac{P(\\text{pos})}{P(\\text{neg})} \\cdot \\prod_{i=1}^{n} \\frac{P(w_i|\\text{pos})}{P(w_i|\\text{neg})})\n",
    "\\Rightarrow \\log\\left(\\frac{P(\\text{pos})}{P(\\text{neg})}\\right) + \\sum_{i=1}^{n} \\log\\left(\\frac{P(w_i|\\text{pos})}{P(w_i|\\text{neg})}\\right)\n",
    "$$\n",
    "\n",
    "So if you have a sentence you want to classify now, you can calculate the $\\lambda$ instead of the ratio.\n",
    "For example if you have the following sentence: <span style=\"color: purple;\">I am happy because I am learning</span> then the table will look like. \n",
    "\n",
    "Summing the $\\lambda$ will give you the likelihood for that tweet being positive or negative\n",
    "\n",
    "| word      | Pos  | Neg  | Î»    |\n",
    "|-----------|------|------|------|\n",
    "| **I**     | 0.05 | 0.05 | 0    |\n",
    "| am        | 0.04 | 0.04 | 0    |\n",
    "| happy     | 0.09 | 0.01 | 2.2  |\n",
    "| because   | 0.01 | 0.01 | 0    |\n",
    "| learning  | 0.03 | 0.01 | 1.1  |\n",
    "| NLP       | 0.02 | 0.02 | 0    |\n",
    "| sad       | 0.01 | 0.09 | -2.2 |\n",
    "| not       | 0.02 | 0.03 | -0.4 |\n",
    "\n",
    "The log likelihood will be the sum of the lambdas. This time wie will have a scale going from -infinity (negative) to infinity (positive). This is also really skewed towards the power words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Naive Bayes\n",
    "\n",
    "Training is different from logistic regression and deep learning as there is no gradient descent.\n",
    "The training is based on counting the frequencies of words in the corpus.\n",
    "Five steps\n",
    "\n",
    "0) Collect and annotate the corpus in positive and negative elements\n",
    "1) Preprocessing similar to the logistic regression, involving \n",
    "    - loweracase\n",
    "    - remove punctuation\n",
    "    - remove stop words\n",
    "    - stemming\n",
    "    - tokenize sentences\n",
    "2) Compute the frequencies of each word in the corpus for positive and negative $freq(w,class)$\n",
    "3) From each word you compute using the [Laplacian Smoothing](#laplacian-smoothing) using the $V_{class}$\n",
    "4) Get the lambda\n",
    "5) Get the log prior as $\\text{logprior} = \\log\\left(\\frac{D_{\\text{pos}}}{D_{\\text{neg}}}\\right)$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntlk_sentiment_analysis-qshMIsGW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
